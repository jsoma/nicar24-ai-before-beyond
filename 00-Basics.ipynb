{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --quiet -O items.zip https://github.com/jsoma/nicar24-beyond-chatgpt/raw/main/items.zip\n",
    "!unzip -o -q items.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the API, let's check out the [OpenAI model playground](https://platform.openai.com/playground?mode=complete).\n",
    "\n",
    "## Working with the API\n",
    "\n",
    "The [chat interface](https://chat.openai.com/) is just one method for interacting with GPT. Another option is to write Python code that skips the website! This is called an **API**. Technically it stands for *Application Programming Interface*, but no one actually remembers that: we can just think of it as a way for two computers to talk directly to each out.\n",
    "\n",
    "To use the OpenAI GPT API with Python, we're going to need to install the [openai Python package](https://github.com/openai/openai-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking questions\n",
    "\n",
    "You use the Python interface just like the chat one: you start from a series of messages and ask the API what's next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the ratio of water to vinegar used when making pickles?\",\n",
    "    }\n",
    "]\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The note above includes the **system prompt**, which are guidelines for GPT that set the tone for the conversation. [You can see the complete ChatGPT one here](https://pastebin.com/vnxJ7kQk) (or rather, one of them), but we're just going to use the traditional \"You are a helpful assistant.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "People who use ChatGPT for free get access to GPT-3.5, while those who pay gain access to GPT-4. GPT-4 can deal with more text at once (a report! a book!) and just generally gives better answers. These are called **models**, and they're the tech that lives behind the interface. You can find more about [the available OpenAI models here](https://platform.openai.com/docs/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='gpt-4'\n",
    " )\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different companies each produce different, competing models: Anthropic has made [Claude](https://claude.ai/), Google has [Bard/Gemini](https://gemini.google.com/), Facebook/Meta made [LLaMA](https://www.llama2.ai/), Mistral has [Mixtral](https://mixtral.replicate.dev/).\n",
    "\n",
    "People have models [fight it out](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) and GPT-4 is currently the best.\n",
    "\n",
    "For many of these models, you're sending information off into the cloud every time you make a request or ask a question. LLaMA is the only current model from the major companies that you can download and run on your own machine! Notice how the [leaderboards](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) start with famous ones that I've listed from big companies - Mixtral, GPT, Gemini â€“ but then descend into things you've never heard of like Vicuna, Yi, and WizardLM.\n",
    "\n",
    "Check the \"License\" column: **they're the non-proprietary ones.**\n",
    "\n",
    "Most concepts can be traferred from model to model, so just know that even if we're working with OpenAI's GPT in the examples below, we could just as well be working with another one (and maybe we'll give one a try later on!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "When you're using Bing's AI assistant, it has three modes: Creative, Balanced or Precise. They're described like so:\n",
    "\n",
    "> - Balanced: best for the most common tasks, like search, maximum speed.\n",
    "> - Creative: whenever you need to generate new content, longer output, more expressive, slower.\n",
    "> - Precise: most factual, minimizing conjectures.\n",
    "\n",
    "You can imagine the system prompt - \"You are a helpful assistant.\" - being changed for each of these behind the scenes.\n",
    "\n",
    "Another change that can be made is **temperature**, which is how \"crazy\" you let your model get. [This Financial Times piece](https://ig.ft.com/generative-ai/) does a good job explaining how it's just a simple matter of statistics: based on the text it's seen so far, what's the most likely next word?\n",
    "\n",
    "The `temperature` setting allows you to use less likely words instead of the most likely next one. Even though it isn't the same as Creative mode, I like to think of them as being similar. Increasing the temperature makes the text more unpredictable, and potentially more creative!\n",
    "\n",
    "By default the temperature with GPT is 0.7, which allows a moderate amount of creativity. If you downgrade the temperature to 0.0, conversations will almost always produce the same result! The maximum is 2.0, which can get some pretty wild results [in the playground](https://platform.openai.com/playground)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a short story in one paragraph.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    " )\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we try the same prompt again, with the same `0.0` temperature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a short story in one paragraph.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    " )\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're tired of being read the same bedtime story every single night, we can increase the temperature to allow GPT to pick less likely words each time it steps forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a short story in one paragraph.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=0.2\n",
    " )\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the further down the text it gets, the further from the original temperature `0.0` version we get! This is because those probabilities add up as you go deeper and deeper into the text, guiding the conversation into completely different paths.\n",
    "\n",
    "And if we want to get something completely different right out of the gate? Let's maximize the temperature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a short story in one paragraph.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=2.0\n",
    " )\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic categorization\n",
    "\n",
    "One of my favorite use cases for using the API is to **put things in categories**, which has the technical term of *classification*. Later we'll look at how to do this with tools that aren't GPT, but let's give it a try for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Categorize the following legislative bill as ENVIRONMENT, HEALTHCARE, IMMIGRATION, TAXES/FINES, or OTHER. \n",
    "\n",
    "Bill text: \n",
    "\n",
    "The \"Celestial Reforestation Act\" proposes the ambitious endeavor of transporting arboreal \n",
    "specimens beyond Earth's confines. Acknowledging the pivotal role of trees in sustaining \n",
    "ecosystems and combating ecological degradation, this bill outlines a strategic roadmap \n",
    "for the selection, launch, and maintenance of arboreal lifeforms into outer space. Through \n",
    "collaboration with aerospace entities and research institutions, this legislation aims to \n",
    "establish extraterrestrial arboreal habitats, facilitating scientific exploration and the \n",
    "expansion of green spaces beyond planetary boundaries. By fostering innovation in space biology\n",
    "and exploration, this act seeks to pioneer sustainable solutions for global challenges while \n",
    "advancing humanity's reach into the cosmos.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    { \"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original response says \"This legislative bill would be categorized as ENVIRONMENT,\" which is *not* okay. I want it to just say the category name!\n",
    "\n",
    "Head on over to [ChatGPT itself](https://chat.openai.com) to engineer a good prompt. Re-run your model until you feel happy with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk processing\n",
    "\n",
    "Oftentimes you end up with a looooong spreadsheet or database of things that need to be categorized. But whether we're technical or not, it's easy to tackle!\n",
    "\n",
    "### Python/pandas\n",
    "\n",
    "> If you're not a Python person: it's fine! Just think about this in terms of concepts. You just want to be familiar with the idea of **api key** and a **prompt**.\n",
    "\n",
    "Let's say we have a dataset that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'title': [\n",
    "        'Trees in space',\n",
    "        'Taxes on people who are in outer space',\n",
    "        'Medical expenses for aliens',\n",
    "        'Pinecones orbiting the planet'\n",
    "    ]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to add a new column to this, called `llm_category`. To run the code above for every row in a pandas dataframe, we make two adjustments to the code above:\n",
    "\n",
    "We build a template for our prompt, which now has a placeholder of `{text}` where our bill details will go:\n",
    "\n",
    "```python\n",
    "prompt_template = \"\"\"\n",
    "Categorize the following legislative bill as ENVIRONMENT, HEALTHCARE, IMMIGRATION, TAXES/FINES, \n",
    "or OTHER. Only respond with the category name.\n",
    "\n",
    "Bill title: {text}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "A function called `llm_request`, which receives a single row of data and uses it to complete the template.\n",
    "\n",
    "```python\n",
    "prompt = prompt_template.format(text=row['title'])\n",
    "```\n",
    "\n",
    "That prompt is then sent to the LLM and the result returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Categorize the following legislative bill as ENVIRONMENT, HEALTHCARE, IMMIGRATION, TAXES/FINES, \n",
    "or OTHER. Only respond with the category name.\n",
    "\n",
    "Bill title: {text}\n",
    "\"\"\"\n",
    "\n",
    "def llm_request(row):\n",
    "    prompt = prompt_template.format(text=row['title'])\n",
    "    \n",
    "    messages = [\n",
    "        { \"role\": \"system\", \"content\": \"You are a legislative assistant.\"},\n",
    "        { \"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try it out with the first row of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = df.loc[0]\n",
    "\n",
    "print(first_row)\n",
    "llm_request(first_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or with a made-up row, just to be able to experiment a little more freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_request({'title': 'The bill to let people from Mars move to planet Earth'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're happy with how it works in small doses, we can move on to using it with every row. We're going to use the Python library [tqdm](https://github.com/tqdm/tqdm) to get some nice progress bars while it chugs along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['llm_category'] = df.progress_apply(llm_request, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's magic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google sheets\n",
    "\n",
    "I personally use [GPT for work](https://gptforwork.com/). I'll show you a demo, but I don't think we can install things on your Google account without getting a little too crazy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the results\n",
    "\n",
    "We can't just say, \"oh wow, computers seem cool, let's just trust whatever it says!\" Our job as journalists who request trust from our audience is to **actually test the results.**\n",
    "\n",
    "We can do this one of two ways:\n",
    "\n",
    "1. Run the classifier over all of our bills, then take a sample of the results to hand-label and compare with the LLM's judgment\n",
    "2. Have a small hand-labeled test dataset that we use to verify the LLM's results before moving on to classify everything.\n",
    "\n",
    "The second path is usually the best since it allows you to tweak your results before running your prompt against *everything*. The first path is useful if you have a workflow already and need to check whether it's [still working](https://www.reddit.com/r/ChatGPT/comments/182ubh7/chatgpt_has_become_unusably_lazy/) or has [shifted unexpectedly](https://arxiv.org/abs/2307.09009)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"labeled-bills.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take our dataset of human-labeled content and see what the AI thinks about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['llm_category'] = df.progress_apply(llm_request, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the two compare? We'll use `crosstab` to do it here, but you can use a pivot table if you're in Excel. It looks complicated, but it's just matching up the `human_label` and `llm_category` column and seeing how often they match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.human_label, df.llm_category, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, environmental bills often got confused for bills about taxes/fines. Healthcare came out with a 100% score\n",
    "\n",
    "> I said \"in this case,\" but the answers actually change if you re-run the LLM category assignment! Even with a small 88-row dataset the LLM will change its mind on some of them.\n",
    "\n",
    "If you were going to do this in Google Sheets, it would be the same thing! You'd just add a `human_label` column, then do a pivot table to see if they match. I [wrote a little Apps Script](https://gist.github.com/jsoma/06783a9e759003e2e69389d677f83c0f) that adds a helper for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardrails\n",
    "\n",
    "Honestly, it did a remarkably good job at returning the results we're looking for! In other situations it's been a little more unpredictable for me. One way to force the AI to return what you're looking for is using a tool like [Guardrails](https://github.com/guardrails-ai/guardrails) to validate responses.\n",
    "\n",
    "> If you aren't technical: using ChatGPT is fine and good because as a human it's easy to understand the response. The difference between getting \"YES, a donut\" as a response and \"Yes - a donut\" is meaningles to a person, but they're very different things when you're a computer! Trying to automatically parse responses when the LLM can decide to go rogue can be a real pain in the neck.\n",
    " \n",
    "There are a LOT of other options for validating/repairing responses, including [LMQL](https://lmql.ai/playground/) and [outlines](https://outlines-dev.github.io/outlines/welcome/). **They are all pretty awful,** though, Guardrails is probably the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet guardrails-ai pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guardrails import validators, Guard\n",
    "from rich import print\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by setting up the structure we'd like our response to be in..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    ${text}    \n",
    "    ${gr.complete_json_suffix_v2}\n",
    "\"\"\"\n",
    "\n",
    "class Comment(BaseModel):\n",
    "    name: str = Field(description=\"Author's name\")\n",
    "    food_item: str = Field(description=\"Food item being discussed, in English\")\n",
    "    email: str = Field(description=\"Author's email\", validators=[\n",
    "        validators.ValidLength(min=1, max=32, on_fail='reask'),\n",
    "        validators.RegexMatch(regex=r'.+@.+', on_fail='reask')\n",
    "    ])\n",
    "    language: str = Field(description=\"Comment language\", validators=[\n",
    "        validators.ValidChoices(\n",
    "            choices=['English', 'Spanish', 'German', 'Other'],\n",
    "            on_fail='reask'\n",
    "        )\n",
    "    ])\n",
    "\n",
    "guard = Guard.from_pydantic(output_class=Comment, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and now we'll run it against a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "After the broccoli incident, I never want to look at broccoli again. Please remove me \n",
    "from the broccoli email list.\n",
    "\n",
    "Sincerely,\n",
    "Jonathan\n",
    "jonathan.soma@gmail.com\n",
    "\"\"\"\n",
    "\n",
    "res = guard(\n",
    "    openai.chat.completions.create,\n",
    "    prompt_params={\"text\": text},\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    max_tokens=4096,\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "# Print the validated output from the LLM\n",
    "print(json.dumps(res.validated_output, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at the conversation it had, we can examine the `guard` object. If we want to have a *fun time* we can intentionally break it by (for example) misspelling a language that the LLM will \"helpfully\" correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard.history.last.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "In this notebook we looked at how AI isn't just one *thing*, it's has all sorts of versions and options same as everything else. Even ChatGPT has different versions - 3.5 and 4, of which 4 is more powerful in a handful of ways (even if slower and more expensive!).\n",
    "\n",
    "A simple task for AI is putting things in categories, also known as classification. When doing bulk classification, it's important to examine the outputs systemtically and not just spot check! That way you know how or why the AI might be going wrong, and either tweak your prompt or build knowledge of the mistakes into your process.\n",
    "\n",
    "Finally, LLMs don't always listen to your rules. They might respond in formats you didn't ask for, add extra categories, or overrule the rules you specified. Validation and repair frameworks like Guardrails re-prompt the LLM to demand it respond in line with your rules, which usually works pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
