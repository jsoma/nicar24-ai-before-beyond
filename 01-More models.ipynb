{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --quiet -O items.zip https://github.com/jsoma/nicar24-beyond-chatgpt/raw/main/items.zip\n",
    "!unzip -o -q items.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More models and tools\n",
    "\n",
    "AI is more than just large language model chatbots!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper\n",
    "\n",
    "OpenAI has released other AI tools besides ChatGPT – one of the most popular is [Whisper](https://openai.com/research/whisper), a model that can **transcribe audio**. The fact, technical name for this is \"speech to text.\"\n",
    "\n",
    "Unlike GPT, **you can actually download and use Whisper**. Python programmers can bop on over to [the GitHub repo](https://github.com/openai/whisper) and coding with it minutes.\n",
    "\n",
    "Because Whisper is an open model (definition *to be discussed*), you'll see all sorts of Whisper-powered tools out there. [MacWhisper](https://goodsnooze.gumroad.com/l/macwhisper) allows you to transcribe audio from the safety of your mac - powered by Whisper! [This random website](https://whisperui.com/) allows to drag-and-drop audio files and transcribe them on the web – powered by Whisper!\n",
    "\n",
    "And now we'll do the exact same thing right here, in Python – powered by Whisper! We'll start by **installing it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --quiet openai-whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like spaCy or the Hugging Face models, Whisper isn't just one piece of software - it's a collection of models with different sizes and names that you have to download separately. When we use `whisper.load_model` below it will run out on the internet and grab the model we're asking for.\n",
    "\n",
    "You can see [the models here](https://github.com/openai/whisper?tab=readme-ov-file#available-models-and-languages). We're going to use `tiny.en`, an English-only model that is the smallest and fastest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"tiny.en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the audio we're going to transcribe. Yes, it's *very* short and not terribly complicated.\n",
    "\n",
    "<audio controls src=\"sample-4.mp3\"></audio>\n",
    "\n",
    "The actual transcribing is just one line! We'll use `%%time` at the top of the cell to see how long it takes, so later we can compare the `tiny.en` model with some other, larger ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = model.transcribe(\"sample-4.mp3\")\n",
    "result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 seconds of audio transcribed in about 1 second! Pretty good, *except* for the fact that it says the incorrect \"We've thrown\" instead of the correct \"We frown.\"\n",
    "\n",
    "Let's try again with a slightly larger model, the medium English-only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"medium.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = model.transcribe(\"sample-4.mp3\")\n",
    "result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing to a slightly larger model really impacted our time! It took 4 seconds for a 2-second audio clip. But on the upside it was at least *correct*.\n",
    "\n",
    "You can try [other models](https://github.com/openai/whisper?tab=readme-ov-file#available-models-and-languages), too. The non-`.en` ones are multilingual (to varying degrees), give them a shot as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see various metrics about how good the transcription abilities are for each language, including CER, WER, BLEU and other scores. One thing to note is that in transcription a 80% score is far worse than a 80% score on, say, a math test. Having one out of every ten words be wrong is... not great in practice.\n",
    "\n",
    "Never listen to scores when dealing with transcription tools, **always test them in the field.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face\n",
    "\n",
    "There are a [zillion models on Hugging Face](https://huggingface.co/models), but the top ones appear to be ALL text generation models. Since we know that [GPT-4 is the best](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) there isn't much use in using any of the other generative text models (...for the moment).\n",
    "\n",
    "By the second or third page you see a few text classification or sentence similarity models, mostly due to the popularity of \"retrieval augmented generation,\" the idea that we can ask a question to an LLM, it finds relevant sentences, then answers a question with them. We don't want to do that, either!\n",
    "\n",
    "As we scroll and scroll and scroll, we eventually come across [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli), which is a text classification model from Facebook. Most normal classification models only know some specific categories to put things in - positive tweets vs negative tweets, for example – but `facebook/bart-large-mnli` can categorize... anything?\n",
    "\n",
    "### Putting things in categories\n",
    "\n",
    "Now that we've settled on [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli), let's install what we need in order to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet transformers torch sentence-transformers sacremoses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model, we're going to click the \"use in Transformers\" link on the top left. That will give us the base code for loading the model with the `pipeline` tool. Then we'll scroll down on the page itself to see if there's an example. And there is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"I'm tired from so much ballet, but it's time to make lunch\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tasks\n",
    "\n",
    "While we're used to just asking questions back and forth with ChatGPT all day, most questions involving language or images are actually pre-defined tasks that have been studies for decades. For example, \"put this text into a category\" is called **classification**.\n",
    "\n",
    "You can see a ton of examples of different machine learning tasks on Hugging Face's [tasks page](https://huggingface.co/tasks).\n",
    "\n",
    "### Translation\n",
    "\n",
    "For example, [translation is one option](https://huggingface.co/tasks/translation). It comes with a [small example](https://huggingface.co/tasks/translation#inference) that seems easy enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "en_fr_translator = pipeline(\"translation_en_to_fr\")\n",
    "en_fr_translator(\"How old are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I showed this to a French person and they *laughed!* It's a word by word translation, not actually how French is spoken. Such is the state of machine learning, *c'est la vie.*\n",
    "\n",
    "> It's probably important to think about how even though you can *an answer* from a tool like this, it doesn't mean it's a *good answer*. It's easy to be distracted by AI seeming fancy and confident, when really it's just a computer pushing numbers around!\n",
    "\n",
    "There's [another example on that page](https://huggingface.co/tasks/translation#inference), but they screwed it up! It uses another model that, if prompted, gives the correct translation of \"How old are you?\". On the page they I guess wanted to mix things up and changed it to translate \"How are you?\". We'll go with what was intended below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "translator(\"How old are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When translating, you request the pipeline `translation_xx_to_yy`, where `xx` is the source language and `yy` is the target language. Not all models support all languages, so you might have to [poke around for what you want](https://huggingface.co/models?pipeline_tag=translation) (the languages tab isn't even always the best route: sometimes the model you want is only filed under \"multilingual\").\n",
    "\n",
    "There are two English-Chinese models that are ranked high, one is [Helsinki-NLP/opus-mt-en-zh](https://huggingface.co/Helsinki-NLP/opus-mt-en-zh) and one is [Helsinki-NLP/opus-mt-zh-en](https://huggingface.co/Helsinki-NLP/opus-mt-zh-en). If we don't read the documentation we won't notice that for going from English to Chinese we need the first one, `opus-mt-en-zh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_zh\", model=\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "translator(\"How old are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the translation in the opposite direction by switching both the model and the pipeline name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_zh_to_en\", model=\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "translator(\"你几岁了?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try [this multilingual model](https://huggingface.co/facebook/nllb-200-distilled-600M) suddenly everything gets very crazy very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\n",
    "    \"translation_ja_to_en\",\n",
    "    model=\"facebook/nllb-200-distilled-600M\")\n",
    "translator(\"私は鉛筆です\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it so bad??? Because despite not telling us how to use the model, `translation_xx_to_yy` apparently is *not* how you use this model, and we apparently need to use [some other weird language codes](https://github.com/facebookresearch/flores/blob/main/flores200/README.md) that we pass in as `src_lang` and `tgt_lang`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\n",
    "    \"translation\",\n",
    "    model=\"facebook/nllb-200-distilled-600M\",\n",
    "    src_lang='jpn_Jpan',\n",
    "    tgt_lang='eng_Latn')\n",
    "translator(\"私は鉛筆です\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I honestly don't know how we were supposed to learn how to do this. I figured it out by reading [the code](https://huggingface.co/spaces/Geonmo/nllb-translation-demo/blob/main/app.py) of one of [the demo spaces](https://huggingface.co/spaces/Geonmo/nllb-translation-demo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "How can you trust anything?? Even if we're impressed by output from AI at first blush, it might not have the consitent quality necessary to perform *real* tasks. Or maybe it does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
