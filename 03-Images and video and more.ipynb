{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --quiet -O items.zip https://github.com/jsoma/nicar24-beyond-chatgpt/raw/main/items.zip\n",
    "!unzip -o -q items.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet timm transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection\n",
    "\n",
    "Object detection does just what it says it does: identifies objects in a picture!\n",
    "\n",
    "[See a zero-shot object detector here](https://huggingface.co/spaces/wendys-llc/object-detection)\n",
    "\n",
    "Let's look at this image:\n",
    "\n",
    "<img src=\"coffee.png\" style=\"max-width: 400px; display: block; margin: auto;\">\n",
    "\n",
    "We'll use the most common model, [facebook/detr-resnet-50](https://huggingface.co/facebook/detr-resnet-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n",
    "\n",
    "pipe(\"coffee.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image segmentation\n",
    "\n",
    "Image segmentation can break apart each individual pixel and assign it a label. Really useful for questions like \"using satellite photography, what percentage of our city is greenery?\"\n",
    "\n",
    "[See an example here](https://huggingface.co/spaces/thiagohersan/maskformer-coco-vegetation-gradio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panoptic segmentation\n",
    "\n",
    "[Another example](https://huggingface.co/spaces/wendys-llc/mask2former-demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image description\n",
    "\n",
    "I'm going to come out strong and say **image descriptions are not good for alt text**. The best alt text is written by someone who knows why the article is written, what is being written about, and why the image is relevant. By understanding that context, the writer of the alt text is able to convey more than just a flat description that might miss important details.\n",
    "\n",
    "...but despite that, [here is a good tweet thread](https://twitter.com/arvindsatya1/status/1674876209543389184) about a dataset about ML and writing captions for data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet clip_interrogator pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip_interrogator import Config, Interrogator\n",
    "ci = Interrogator(Config(clip_model_name=\"ViT-L-14/openai\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"coffee.png\").convert('RGB')\n",
    "\n",
    "ci.interrogate(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the big reasons for using image descriptions is *trying to make similar images with AI*. OpenAI offers a model called CLIP that you can use to (somewhat) reverse-engineer the prompt that was used to create an image.tr\n",
    "\n",
    "[Check out the CLIP Interrogator](https://huggingface.co/spaces/pharmapsychotic/CLIP-Interrogator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table and form analysis\n",
    "\n",
    "As journalists, *we love to complain about PDFs*. While we might be used to using tools like Tabula to wrangle CSVs out of them, the world has moved on to newer, nicer tools. Most of the time it has to do with invoices: auto-detecting tables, OCR, and figuring out what different parts of the page are (the total, the items, the billing address) are now reasonable possible with open-source models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Speaking of table extraction: if you haven't seen [pdfplumber](https://github.com/jsvine/pdfplumber), it's a *delight*. I made [a Hugging Face demo](https://huggingface.co/spaces/wendys-llc/pdfplumber-demo) of it even though it isn't AI-powered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image-based Q&A\n",
    "\n",
    "Asking questions about an image is related to all of the above image-describing bits: it can only answer questions that involve things that can be identified/described by CLIP/object detection/etc.\n",
    "\n",
    "[Here's a great example comparing different models](https://huggingface.co/spaces/wendys-llc/comparing-VQA-models)\n",
    "\n",
    "Another part of that is **document-based Q&A**, which is mostly \"let's look at invoices.\" [You can see the most popular model here](https://huggingface.co/impira/layoutlm-document-qa), and the one that's [fine-tuned for invoices here](https://huggingface.co/impira/layoutlm-invoices).\n",
    "\n",
    "Let's take a look at this invoice:\n",
    "\n",
    "<img src=\"invoice-template-us-neat-750px.png\" style=\"width: 300px; margin: auto;\">\n",
    "\n",
    "Along with the model, we do need to install `pytesseract` to analyze text on the page. You'll need to install tesseract as `apt install tesseract-ocr` on Colab or `brew install tesseract`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"document-question-answering\", model=\"impira/layoutlm-document-qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(\n",
    "    \"invoice-template-us-neat-750px.png\",\n",
    "    \"What is the invoice number?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use full URLs, the image is from [here](https://templates.invoicehome.com/invoice-template-us-neat-750px.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But when you're working with PDFs**, you probably want to use the [docquery](https://github.com/impira/docquery) library, which is basically the same, but a little easier to use. Sadly it needs an old version of pydantic and transformers and will probably cause all sorts of trouble. Hopefully you use virtual environments! It also isn't actively maintained anymore. *But* it works with PDFs!\n",
    "\n",
    "> You'll need to run `brew install poppler` on macs, `apt install poppler-utils` is necessary on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet \"docquery\" pydantic==1.10 transformers==4.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the same document above, but as a PDF with a text layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docquery import document, pipeline\n",
    "\n",
    "p = pipeline('document-question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = document.load_document(\"invoice.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p(question=\"What is the invoice number?\", **doc.context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p(question=\"What is the sales tax rate?\", **doc.context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p(question=\"What are the items on the invoice?\", **doc.context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classification (doing it yourself)\n",
    "\n",
    "[Head over to here](https://huggingface.co/autotrain)! Autotrain is in the middle ground between \"the most convenient tool\" and \"sometimes it breaks and you're confused about how to fix it.\" You can [see how to use it with image classification here](https://huggingface.co/docs/autotrain/image_classification). **You need to changed mixed_precision from `fp12` to `false` in order for it to work!**\n",
    "\n",
    "In the example below, we're classifying amber mines like [this piece from Texty](https://texty.org.ua/d/2018/amber_eng/). It used to be the case of [a whole set of education materials about machine learing](https://newsinitiative.withgoogle.com/resources/trainings/hands-on-machine-learning/investigating-stories-with-machine-learning/) from Google News Initiative, but now you can reproduce it all *in all of five minutes*.\n",
    "\n",
    "After you use the autotrainer it build you a new model, [like this one](https://huggingface.co/wendys-llc/autotrain-stfol-259iu). It'll be named something awful, it won't give you an alert or anything, it will just magically appear and your space will shut down. You'll need to make the model public, then you can do something similar to the below.\n",
    "\n",
    "Images to test: [this](51.52066320000001_26.477740000000196_09_Mar_2015_GMT_super_9.png) and [this](51.51732900000001_26.461450000000195_09_Mar_2015_GMT_super_6.png) and [this](51.61886970000046_27.210790000000262_05_Oct_2015_GMT_super_8.png) and [this](51.46398180000005_26.31484000000018_09_Mar_2015_GMT_super_10.png) and [this](51.601393_26.555139_09_Mar_2015_GMT_super_24.png) and [this](51.46731600000005_26.320270000000182_09_Mar_2015_GMT_super_20.png). It works magically and we only have 25 examples of each!\n",
    "\n",
    "> We're going to reinstall transformers so that we make sure we have the most recent one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --quiet transformers timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"image-classification\", model=\"wendys-llc/autotrain-stfol-259iu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 51.52066320000001_26.477740000000196_09_Mar_2015_GMT_super_9.png\n",
    "# 51.51732900000001_26.461450000000195_09_Mar_2015_GMT_super_6.png\n",
    "# 51.61886970000046_27.210790000000262_05_Oct_2015_GMT_super_8.png\n",
    "# 51.46398180000005_26.31484000000018_09_Mar_2015_GMT_super_10.png\n",
    "# 51.601393_26.555139_09_Mar_2015_GMT_super_24.png\n",
    "# 51.46731600000005_26.320270000000182_09_Mar_2015_GMT_super_20.png\n",
    "\n",
    "from PIL import Image\n",
    "image = Image.open(\"51.601393_26.555139_09_Mar_2015_GMT_super_24.png\")\n",
    "classifier(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format shifting\n",
    "\n",
    "A valuable concept for making use of all these new tools is *format shifting*, the idea that we can convert willy-nilly between different types of media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio to text\n",
    "\n",
    "Speech to text – transcription – is something we've covered at length already. Using OpenAI's [Whisper](https://github.com/openai/whisper) we're easily able to convert from audio to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --quiet openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"tiny.en\")\n",
    "result = model.transcribe(\"sample-4.mp3\")\n",
    "result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to speech\n",
    "\n",
    "[Elevenlabs](https://elevenlabs.io/) is my favorite tool for doing voice work. I (unfortunately) haven't found an open-source model that does nearly as well. Point it to five minutes of video and *voilà*, you have the ability to create oddly lifelike speech from a piece of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech to ...speech?\n",
    "\n",
    "One of the problems with text-to-speech is it *probably* won't capture the intonation, the feeling, the stress we're looking for. If we're especially picky about how good our communication is, we can use a speech-to-speech model! A speech-to-speech model can basically mask our voice with someone else's, maintaining all of those emotional characteristics that are lost in the text-to-speech process.\n",
    "\n",
    "You can also use [Elevenlabs](https://elevenlabs.io/) for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video to images\n",
    "\n",
    "The easiest way to process video is *not* looking for a video-specific model. Instead, you just cut up your video into a collection of segments and run image analysis on each and every one of them!\n",
    "\n",
    "> `!brew install ffmpeg` for macs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -i sora-dog.mp4 -vf fps=1 output%d.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we make it, we could use an **object detection model** to see what's inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --quiet transformers timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"output1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n",
    "\n",
    "pipe(\"output1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While rare, there *are* a few video-specific models. They might do **scene segmentation**, splitting videos into chunks, or **speaker identification**, which uses cues like face detection and whose mouth is moving to identify who on the audio track is speaking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object tracking in videos\n",
    "\n",
    "https://github.com/tryolabs/norfair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to video\n",
    "\n",
    "OpenAI's [Sora](https://openai.com/sora) is the new entrant to generative video, and it looks *great*. Previously folks were excited about [Runway ML](https://runwayml.com/) buuuuut Sora blows it out of the water."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image to 3D\n",
    "\n",
    "3D is going at a breakneck speed, but much of it is based around tools as opposed to models. You can [see a Hugging Face demo here](https://huggingface.co/spaces/zxhezexin/OpenLRM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "Image- and video-based models aren't nearly as easy to use as the text-based models! They're sometimes great but at other points fall flat on their face. They're also kind of tough to install and wrangle from a software point of view.\n",
    "\n",
    "Fun, though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
